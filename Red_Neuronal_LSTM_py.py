# -*- coding: utf-8 -*-
"""Prediccion de acciones de mercado con Red Neuronal Recurrente LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10nJ0_TKRVvA8gQ4eVLc0L7pxOHiCYW9c

Este proyecto se ha logrado con la ayuda de este tutorial para [LSTM Neural Network](https://www.codificandobits.com/blog/tutorial-prediccion-de-acciones-en-la-bolsa-redes-lstm/)

# Desarrollo de una Red Neuronal LSTM para la prediccion de valores de acciones

#### Carga librerias necesarias
"""

# Manipulacion de archivos
from google.colab import files

# Manipulacion de datos
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler  # normalizacion de los datos

# Visualizacion de datos
import matplotlib.pyplot as plt

# Modelos de aprendizaje
from keras.models import Sequential
from keras.layers import LSTM, Dense

"""#### Carga el dataset para trabajar"""

files.upload()

"""#### Funciones auxiliares"""

def plot_prediccion(datos_reales, datos_predichos):
    """
    Grafica la evolución de los datos reales y los datos predichos.

    Args:
        datos_reales (list): Lista de valores reales.
        datos_predichos (list): Lista de valores predichos.
    """

    # Graficar los datos reales en azul y los datos predichos en rojo
    diferencia_entre_bloques = len(datos_reales)-len(datos_predichos)
    plt.plot(datos_reales[diferencia_entre_bloques:], color='blue', label='Datos Reales')
    plt.plot(datos_predichos, color='red', label='Datos Predichos')

    # Agregar etiquetas y leyenda
    plt.ylim(1.1 * np.min(datos_predichos)/2, 1.1 * np.max(datos_predichos))
    plt.xlabel('Tiempo')
    plt.ylabel('Valor de la acción')
    plt.legend()

    # Mostrar el gráfico
    plt.show()

"""#### Lectura de los datos"""

dataset = pd.read_csv('AAPL_2006-01-01_to_2018-01-01.csv', index_col='Date', parse_dates=['Date'] )
print(dataset.shape)
dataset.head()

dataset.tail()

"""El dataset comprende una secuencia temporal, los registros empiezan el 2006-03-01 y terminan el 2017-12-29. En total registras los valores de la accion de, aproximadamente, 12 años."""

dataset.info()

"""No hay valores nulos ni valores faltantes.
El dataset cuenta con 5 columnas numericas (4 float y 1 int) y 1 columna de tipo categórica
"""

dataset.describe()

dataset.describe(include=['O'])

"""### Split de los datos

Dado que es una secuencia temporal, separo los datos del final para usarlos como datos de validacion.
Entonces, todos los registros hasta el año 2016, incluido, serán para el entrenamiento y el último año, 2017, será para hacer la validación.
Solamente se utilizará un valor de la acción en cada día, en este caso, el valor máximo, 'High'.
"""

set_entrenamiento = dataset.loc[:'2016','High']

"""Otra forma de escribir el comando para separarlo es  `dataset['High'][:'2016']`. En estos dos casos, el tipo de dato retornado es Serie.

En cambio, otra forma de extraer los datos para el entrenamiento es `dataset[:'2016'].iloc[:,1:2]` en este caso el tipo de dato es distinto, devuelve un Dataframe de una sola columna.
"""

set_validacion = dataset.loc['2017':,'High']  # de nuevo, otra forma es dataset['High']['2017'], o dataset['2017':].iloc[:,1:2]

"""#### Visualizacion de la separación"""

set_entrenamiento.plot(legend=True)
set_validacion.plot(legend=True)

plt.legend(['Entrenamiento (2006-2016)', 'Validación (2017)'])
plt.show()

"""### Normalización de los datos

Se escalan los datos para evitar el conflicto que genera la secuencia creciente de datos. La función de costo daría mayor importancia a los valores más alto de la secuencia y afectaría negativamente el rendimiento del modelo.

Para las series temporales no se suele estandarizar los datos, en cambio, se los escala.
"""

sc = MinMaxScaler(feature_range=(0,1))
set_entrenamiento_escalado = sc.fit_transform(pd.DataFrame(set_entrenamiento))
set_entrenamiento_escalado.shape

set_validacion_escalado = sc.transform(pd.DataFrame(set_validacion))

"""Para visualizar correctamente, se convierte los datasets escalados a dataframes con los indices originales para que Pandas grafique ambos en el mismo cuadro."""

# Convierte a dataframe
set_entrenamiento_escalado_df = pd.DataFrame(set_entrenamiento_escalado)
set_validacion_escalado_df = pd.DataFrame(set_validacion_escalado)

# Convierte los indices
set_entrenamiento_escalado_df.index = set_entrenamiento.index
set_validacion_escalado_df.index = set_validacion.index


set_entrenamiento_escalado_df[0].plot(legend=True)
set_validacion_escalado_df[0].plot(legend=True)

plt.legend(['Entrenamiento', 'Validacion'])
plt.title('Datos Escalados')
plt.show()

"""### Creacion y entrenamiento del modelo

#### Definicion del tamano de los datos para el modelo

Para el entrenamiento se tomarán bloques de datos consecutivos. Es decir, sera una secuencia de bloques de datos donde cada bloque se usara para predecir el dato inmediatamente siguiente.

Por ejemplo, si el primer bloque de entrenamiento comprende 60 datos, luego el primer valor de la secuencia de predicciones correspondera al valor numero 61 en el orden de la secuencia. Asi, hasta el final de la secuencia de entrenamiento. Quedando dos secuencias con un `length` de 60 unidades menor al del set original, es decir, el `length` del bloque.
"""

time_step = 60  # tamaño del bloque
m = len(set_entrenamiento_escalado) # limite de la iteracion
X_train = []
Y_train = []

for i in range(time_step, m):
  X_train.append(set_entrenamiento_escalado[i-time_step:i,0])

  # el dato inmediatamente siguiente al del bloque
  Y_train.append(set_entrenamiento_escalado[i,0])

print('Length de X_train:',len(X_train))

# Se devuelve el formato de dato anterior para seguir trabajando
X_train, Y_train = np.array(X_train), np.array(Y_train)

print('Dimension de X_train reconvertido a array:', X_train.shape)
print('Dimension de Y_train reconvertido a array:', Y_train.shape)

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_train.shape

"""#### Creacion del modelo

Se construye un modelo `Sequential` *'many-to-one'* comprendido por dos capas internas.

La primera es una red `LSTM`, especial para casos de secuencias larga de datos como este.

La segunda, la capa de salida, `Dense` que retorna una salida numerica directa sin aplicar una funcion de activacion, ideal para tareas de regresion como esta donde se busca predecir un valor numerico continuo.
"""

dim_entrada = (X_train.shape[1],1) # (timesteps, features)
dim_salida = 1
na = 50 # número de unidades (neuronas)

modelo = Sequential()

# Capa LSTM
modelo.add(LSTM(units=na, input_shape=dim_entrada))

# Capa de Salida (sin activación para regresión)
modelo.add(Dense(units=dim_salida))

# Compilacion
modelo.compile(optimizer='rmsprop', loss='mse')

"""Se utiliza en la compilacion el optimizador `rmsprop` para acelerar la convergencia.

La funcion de costo es el error cuadratico medio, `mse`.

#### Entrenamiento del modelo
"""

modelo.fit(X_train, Y_train, epochs=20, batch_size=32)

"""### Prediccion

#### Definicion del tamano de los datos para la validacion

Primero hay que preparar el set de validacion con la misma estructura de bloques con la que se trabajo para el entrenamiento.
"""

X_test = []
for i in range (time_step, len(set_validacion_escalado)):
  X_test.append(set_validacion_escalado[i-time_step:i,0])

X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

X_test.shape

"""#### Prediccion"""

prediccion = modelo.predict(X_test)

"""Se invierte la normalizacion para obtener los valores predichos en la escala real de las acciones."""

prediccion = sc.inverse_transform(prediccion)

"""#### Visualizacion

Es importante notar que la prediccion se hace desde el primer valor despues del primer bloque. En el caso del ejemplo, es el dato de orden 61 en la secuencia.
"""

plot_prediccion(set_validacion.values, prediccion)

"""### Analisis posterior

Se cargan los valores a un nuevo DataFrame ajustando el tamano del set de validacion al tamano de la prediccion, `len(prediccion)`
"""

set_validacion.shape

len(prediccion)

res_set_validacion = set_validacion[-len(prediccion):]
res_set_validacion.index

res = pd.DataFrame({'High validacion':res_set_validacion,'High prediccion':prediccion[:,0]})
res.head()

res['Error'] = ((res['High validacion']-res['High prediccion'])/res['High validacion']) * 100
res.head()

res['Error'].plot(legend=True)

plt.title('Error validacion-prediccion')
plt.xlabel('Tiempo')
plt.ylabel('Porcentaje')

"""Se puede observar que el error no es mayor al 6% casi en la totalidad de los valores predichos.


"""

res[(res['Error']>6) | (res['Error']<-6)].sort_values('Error', ascending=False)

"""Solamente hay 4 ocasiones en las que el error de prediccion supera el 6%, siendo 7.14% el maximo error."""

res['Error'].hist()

"""El sesgo del error es positivo, lo que se traduce en que el modelo tiende a predecir los valores por debajo del valor real. En la gran mayoria de los casos el valor predicho se encuentra entre 1 a 3% por debajo del valor real."""